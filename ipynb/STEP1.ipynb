{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEFIT\n",
    "\n",
    "This would be the first attempt to run the unsupervised learning VAE network to learn how to characterize a 1D profile with atted noise and missing input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as colors \n",
    "\n",
    "import ipysh\n",
    "import Hunch_utils  as Htls\n",
    "import Hunch_lsplot as Hplt\n",
    "\n",
    "%aimport Dummy_g1data\n",
    "import Dummy_g1data as dummy\n",
    "\n",
    "%aimport models.base\n",
    "%aimport models.AEFIT\n",
    "# ipysh.Bootstrap_support.debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Model\n",
    "The model and data generator are set:\n",
    "Dummy data generator generates from a set of 5 kind of curves with a dataset cardinality of 10K samples.\n",
    "\n",
    "All the shapes are generated from a dictionary array that defines mean sigma and gain of sum of gaussians.\n",
    "This table is printed from the variable ds.kinds\n",
    "\n",
    ">NOTE: \n",
    "> The actual model is generating random so it is not redoing the very same samples on each epoch.\n",
    "> To exactly constraint the maximum size of the dataset the buffer can be used\n",
    "\n",
    "the model uses by default an input of 40 samples that are the (x,y) tuple values of 20 points from the generated shapes.\n",
    "If the command **buffer()** is used all shaped are stored in a buffer and the generator yields always the same set of curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dummy.Dummy_g1data(counts=10000).buffer()\n",
    "ds.kinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataset to a mirrror data-data suitable to be fed into VAE\n",
    "dds = ds.ds_array.map(lambda xy,l: (xy,xy) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a VAE model from AEFIT prototype\n",
    "m = models.AEFIT.AEFIT(latent_dim=2, scale=1, beta=0., geometry=[20,20,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.beta.assign(0.0005)\n",
    "fit = lambda: m.fit(dds.skip(3000).batch(100, drop_remainder=True), validation_data=dds.take(3000).batch(100), epochs=39, shuffle=False)\n",
    "# If interactive notebook\n",
    "models.base.fn_thread(m3, fit).control_panel()\n",
    "\n",
    "# If standard notebook\n",
    "# fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starts a viewer of the latentspace\n",
    "p = Hplt.LSPlotBokeh()\n",
    "p.set_model(m)\n",
    "p.set_data(ds, counts=1000)\n",
    "p.plot(notebook_url='http://172.17.0.2:8888')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this plot the relevance layer \n",
    "relevance = m.generative_net.layers[0]\n",
    "relevance.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data simulation\n",
    "\n",
    "Now we want to test the network against particular shapes within the latent main paths but with added noise and simulated missing data. The function simulate_missing_data reduce de number of available input simply duplicating the point that precedes the missing one with the same value.\n",
    "\n",
    "A further gaussian noise has been also applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_missing_data(m, lpt=[0.5,-1.6], noise_var=0.05, arr = []):\n",
    "    xy = m.decode(tf.convert_to_tensor([pt]), training=False)\n",
    "    x,y = tf.split(xy[0], 2)\n",
    "    x,y = (x.numpy(), y.numpy())\n",
    "\n",
    "    fig = plt.figure('gen_missing_curve',figsize=(18, 6))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)    \n",
    "    \n",
    "    ax1.set_xlim(-2.,2.)\n",
    "    ax1.set_ylim(-2.,2.)\n",
    "    \n",
    "    ax1.scatter(pt[0],pt[1],s=80)\n",
    "    ax2.scatter(x,y,s=40)\n",
    "\n",
    "    # apply noise\n",
    "    x += np.random.normal(0,noise_var,len(x))\n",
    "    y += np.random.normal(0,noise_var,len(y))\n",
    "\n",
    "    # apply missing data simulation\n",
    "    for i,v in enumerate(arr,0):\n",
    "        x[arr[i]]=x[arr[i]+1]\n",
    "        y[arr[i]]=y[arr[i]+1]\n",
    "    \n",
    "    ax2.scatter(x,y,s=80)\n",
    "\n",
    "    me,va = m.encode(tf.reshape(tf.concat([x,y],0), shape=[1,-1]), training=False)\n",
    "    print(\"Guessed Latent point = \",me.numpy())\n",
    "    gpt = me[0].numpy()\n",
    "    ax1.scatter(gpt[0],gpt[1])\n",
    "    \n",
    "    XY = m.decode(me, training=False)\n",
    "    X,Y = tf.split(XY[0], 2)\n",
    "    X,Y = (X.numpy(), Y.numpy())\n",
    "    # plt.figure('reconstructed')\n",
    "    ax2.scatter(X,Y,s=40)\n",
    "    # plt.plot(X,Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by generating close to the shape {'mean': [0.5], 'sigma': [0.2], 'gain': [0.5]}\n",
    "\n",
    "This is in the middle of the central cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from point: 0.6, -0.7\n",
    "pt = [-0.400,0.593]\n",
    "noise_var = 0.1\n",
    "arr = [3,2,1,5,8,7,6,9,12,11,14,13,18]\n",
    "simulate_missing_data(m, pt,noise_var,arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to check if the nework can simulate a point in the middle od two clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from point: 0.5, -1.6\n",
    "pt = [0.666,-0.278]\n",
    "noise_var = 0.05\n",
    "arr = [3,2,1,5,8,7,6,9,12,11,14,13,18]\n",
    "simulate_missing_data(m, pt,noise_var,arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from point: 0.5, -1.6\n",
    "pt = [-1.283,0.541]\n",
    "noise_var = 0.0\n",
    "#arr = [3,2,1,5,8,7,6,9,12,11,14,13,18]\n",
    "simulate_missing_data(m, pt,noise_var)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}